{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e180ccc31576322",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-13T21:10:06.390919400Z",
     "start_time": "2024-03-13T21:10:06.298142300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection established.\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import logging\n",
    "\n",
    "DB_NAME = 'blockchain'\n",
    "DB_USER = 'postgres'\n",
    "DB_PASSWORD = 'auth2020'\n",
    "DB_HOST = 'localhost'\n",
    "DB_PORT = '5432'\n",
    "\n",
    "\n",
    "try:\n",
    "    # Create a PostgreSQL connection\n",
    "    conn = psycopg2.connect(\n",
    "        dbname=DB_NAME, user=DB_USER, password=DB_PASSWORD, host=DB_HOST, port=DB_PORT\n",
    "    )\n",
    "    cur = conn.cursor()\n",
    "    print(\"Connection established.\")  # Log successful connection (you can modify this)\n",
    "\n",
    "    # Configure logging to write to a log file\n",
    "    logging.basicConfig(filename='log.txt', level=logging.INFO)\n",
    "    logging.info(\"Connection established with Database.\")  # Log the same message to the file\n",
    "\n",
    "\n",
    "    # Additional code for database operations goes here...\n",
    "\n",
    "except Exception as err:\n",
    "    # Handle exceptions\n",
    "    print(\"Unable to connect to database.\")  # Log error (you can modify this)\n",
    "    print(err)  # Print the specific error message\n",
    "\n",
    "    # Optionally, write the error message to a file (error.txt)\n",
    "    with open('error.txt', 'a') as error_file:\n",
    "        error_file.write(str(err) + '\\n')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "fb031dd6bc63e996"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'C:/Users/lmhmo/Indexer_Project/indexer_project/Decrypted_files/5905903.json' exists. Proceeding with further actions.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "\n",
    "block = '5905903'  # Change this number if you want to load a different file\n",
    "file_path = f'C:/Users/lmhmo/Indexer_Project/indexer_project/Decrypted_files/{block}.json'\n",
    "\n",
    "try:\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"File '{file_path}' exists. Proceeding with further actions.\")\n",
    "        # Add your additional code here\n",
    "\n",
    "        # Configure logging to write to both log.txt and error.txt\n",
    "        logging.basicConfig(filename='log.txt', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "        logging.info(f\"File '{file_path}' exists. Proceeding with further actions.\")\n",
    "\n",
    "    else:\n",
    "        print(f\"File '{file_path}' does not exist.\")\n",
    "        # Log the error message to error.txt\n",
    "        with open('error.txt', 'a') as error_file:\n",
    "            error_file.write(f\"File '{file_path}' does not exist.\\n\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    # Log the error message to error.txt\n",
    "    with open('error.txt', 'a') as error_file:\n",
    "        error_file.write(f\"An error occurred: {e}\\n\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-13T21:10:08.848645900Z",
     "start_time": "2024-03-13T21:10:08.834856600Z"
    }
   },
   "id": "8fae7717f924d9db",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       @type source_port source_channel  \\\n",
      "0  /ibc.applications.transfer.v1.MsgTransfer    transfer      channel-5   \n",
      "\n",
      "                                           sender  \\\n",
      "0  migaloo17elxaps3vgzq4nrturr55c0z3rsn32nyzg8zg4   \n",
      "\n",
      "                                      receiver    timeout_timestamp memo  \\\n",
      "0  osmo17elxaps3vgzq4nrturr55c0z3rsn32ny88agtf  1710188661779000000        \n",
      "\n",
      "  token.denom token.amount timeout_height.revision_number  ...  \\\n",
      "0      uwhale   1852877000                              0  ...   \n",
      "\n",
      "                  public_key.@type  \\\n",
      "0  /cosmos.crypto.secp256k1.PubKey   \n",
      "\n",
      "                                 public_key.key mode_info.single.mode   denom  \\\n",
      "0  A7JU+NEqpAsGErxxsDWu1IDtVFew2RNY5aIqvJQFZ0Kt      SIGN_MODE_DIRECT  uwhale   \n",
      "\n",
      "   amount                                         signatures gas_limit payer  \\\n",
      "0  150000  DthFHlVbz5af6ZFRmamVOjLlqh/0s3NjQYDrRJ8HPMcOmF...    150000         \n",
      "\n",
      "  granter   tip  \n",
      "0          None  \n",
      "\n",
      "[1 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "# This section of code uses json and pandas to clean json file and get it ready to sort it into a tabular format to prepare for postgres\n",
    "import pandas as pd\n",
    "import json\n",
    "# Specify the file path\n",
    "\n",
    "with open(file_path, 'r') as json_file:\n",
    "    json_data = json.load(json_file)\n",
    "\n",
    "# Slight modifications made to make data postgres friendly\n",
    "def replace_empty_strings(item):\n",
    "    return \"N/A\" if item == \"null\" else item\n",
    "\n",
    "for field in [\"sender\", \"receiver\", \"memo\"]:\n",
    "    json_data[\"tx\"][\"body\"][\"messages\"][0][field] = replace_empty_strings(json_data[\"tx\"][\"body\"][\"messages\"][0][field])\n",
    "# This section of code calls the paths of all the attributes needs for the postgres table\n",
    "df_messages = pd.json_normalize(json_data[\"tx\"][\"body\"][\"messages\"])\n",
    "df_auth_info = pd.json_normalize(json_data[\"tx\"][\"auth_info\"][\"signer_infos\"])\n",
    "df_fee_info = pd.json_normalize(json_data[\"tx\"][\"auth_info\"][\"fee\"][\"amount\"])\n",
    "df_sig_info = pd.DataFrame({\"signatures\": json_data[\"tx\"][\"signatures\"]})\n",
    "df_gas_info = pd.DataFrame({\"gas_limit\": [json_data[\"tx\"][\"auth_info\"][\"fee\"][\"gas_limit\"]]})\n",
    "df_payer_info = pd.DataFrame({\"payer\": [json_data[\"tx\"][\"auth_info\"][\"fee\"][\"payer\"]]})\n",
    "df_granter_info = pd.DataFrame({\"granter\": [json_data[\"tx\"][\"auth_info\"][\"fee\"][\"granter\"]]})\n",
    "df_tip_info = pd.DataFrame({\"tip\": [json_data[\"tx\"][\"auth_info\"][\"tip\"]]})\n",
    "\n",
    "# Combine dataframes\n",
    "df_combined = pd.concat(\n",
    "    [df_messages, df_auth_info, df_fee_info, df_sig_info, df_gas_info, df_payer_info, df_granter_info, df_tip_info],\n",
    "    axis=1)\n",
    "\n",
    "# Display the combined dataframe\n",
    "print(df_combined)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-13T21:10:09.365712800Z",
     "start_time": "2024-03-13T21:10:09.331974800Z"
    }
   },
   "id": "c66771214f5e6a89",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Creates tabular format\n",
    "column_names = list(df_combined.columns)\n",
    "# Slight modifications that needed to be made to prepare for postgres\n",
    "def clean_column_name(col): # Special symbols can not be inside of column names\n",
    "    return col.replace(\"@\", \"\").replace(\" \", \"_\").replace('.', '')\n",
    "\n",
    "column_names_cleaned = [clean_column_name(col) for col in column_names]\n",
    "\n",
    "message = json_data[\"tx\"][\"body\"][\"messages\"][0]\n",
    "table_name = message[\"@type\"].split(\".\")[-1][-10:]\n",
    "table_name = table_name.lower()  # This has to be specified for check_table_query to work"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-13T21:10:10.058304800Z",
     "start_time": "2024-03-13T21:10:10.042791900Z"
    }
   },
   "id": "4ed0473d4a0432f5",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 'sgtransfer' already exists. Data will be appended to the existing table.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a cursor\n",
    "\n",
    "\n",
    "# Check if the table already exists\n",
    "check_table_query = f\"\"\"\n",
    "    SELECT EXISTS (\n",
    "        SELECT 1\n",
    "        FROM pg_tables\n",
    "        WHERE schemaname = 'public' AND tablename = '{table_name}'\n",
    "    );\n",
    "\"\"\"\n",
    "\n",
    "cur.execute(check_table_query)\n",
    "table_exists = cur.fetchone()[0]  # Fetch the result of the query\n",
    "\n",
    "if table_exists:\n",
    "    print(f\"Table '{table_name}' already exists. Data will be appended to the existing table.\")\n",
    "else:\n",
    "    print(f\"Table '{table_name}' does not exist. Creating a new table...\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-13T21:10:10.623765500Z",
     "start_time": "2024-03-13T21:10:10.564464800Z"
    }
   },
   "id": "5801cceed630449e",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 'sgtransfer' already exists. Data will be appended to the existing table.\n",
      "                                       @type source_port source_channel  \\\n",
      "0  /ibc.applications.transfer.v1.MsgTransfer    transfer      channel-5   \n",
      "\n",
      "                                           sender  \\\n",
      "0  migaloo17elxaps3vgzq4nrturr55c0z3rsn32nyzg8zg4   \n",
      "\n",
      "                                      receiver    timeout_timestamp memo  \\\n",
      "0  osmo17elxaps3vgzq4nrturr55c0z3rsn32ny88agtf  1710188661779000000        \n",
      "\n",
      "  token.denom token.amount timeout_height.revision_number  ...  \\\n",
      "0      uwhale   1852877000                              0  ...   \n",
      "\n",
      "                  public_key.@type  \\\n",
      "0  /cosmos.crypto.secp256k1.PubKey   \n",
      "\n",
      "                                 public_key.key mode_info.single.mode   denom  \\\n",
      "0  A7JU+NEqpAsGErxxsDWu1IDtVFew2RNY5aIqvJQFZ0Kt      SIGN_MODE_DIRECT  uwhale   \n",
      "\n",
      "   amount                                         signatures gas_limit payer  \\\n",
      "0  150000  DthFHlVbz5af6ZFRmamVOjLlqh/0s3NjQYDrRJ8HPMcOmF...    150000         \n",
      "\n",
      "  granter   tip  \n",
      "0          None  \n",
      "\n",
      "[1 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# %%\n",
    "\n",
    "if not table_exists:\n",
    "    # Create the table with dynamic column names (matching cleaned attributes)\n",
    "    create_table_query = f\"\"\"\n",
    "        CREATE TABLE {table_name} (\n",
    "            id SERIAL PRIMARY KEY,\n",
    "            {', '.join(f\"{column_names_cleaned[i]} VARCHAR(255)\" for i in range(len(column_names)))}, block INTEGER NOT NULL\n",
    "        );\n",
    "    \"\"\"\n",
    "    cur.execute(create_table_query)\n",
    "    print(f\"Table '{table_name}' created successfully.\")\n",
    "else:\n",
    "    print(f\"Table '{table_name}' already exists. Data will be appended to the existing table.\")\n",
    "\n",
    "# Insert data into the table\n",
    "for _, row in df_combined.iterrows():\n",
    "    # Assuming you have already defined the variable block_number\n",
    "    # Modify the INSERT query to include the block number\n",
    "    insert_query = f\"\"\"\n",
    "        INSERT INTO {table_name} ({', '.join(column_names_cleaned)}, block)\n",
    "        VALUES ({', '.join(f\"'{row.iloc[i]}'\" for i in range(len(column_names)))}, {block});\n",
    "    \"\"\"\n",
    "cur.execute(insert_query)\n",
    "\n",
    "# Commit changes\n",
    "conn.commit()\n",
    "\n",
    "# Close the cursor and connection\n",
    "cur.close()\n",
    "conn.close()\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "print(df_combined)\n"
   ],
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-13T21:10:12.534061800Z",
     "start_time": "2024-03-13T21:10:12.514484900Z"
    }
   },
   "id": "initial_id",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "7e5dbdd45681e8f2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
